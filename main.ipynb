{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c53c56f",
   "metadata": {},
   "source": [
    "# Influencia de variables adicionales en la predicción de los niveles de glucosa en sangre.\n",
    "\n",
    "Trabajo de Finalización de Máster en Inteligencia Artificial en la [Universidad Internacional de la Rioja (UNIR)](https://unir.net)\n",
    "\n",
    "Realizado por [Xavi Coret Mayoral](https://xcoret.github.io/portfolio/)\n",
    "\n",
    "El conjunto de datos utilizado (\"Glucose_measurements_sample.csv\") ha sido proporcionado por el director de este Trabajo de Fin de Máster, Ciro Rodríguez León."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697950f7",
   "metadata": {},
   "source": [
    "## Instalación e importación de librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c202c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization,CuDNNLSTM\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58daa7",
   "metadata": {},
   "source": [
    "## Definición de constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquetas de los meses del año\n",
    "MONTH_TAGS = ['Enero','Febrero','Marzo','Abril','Mayo','Junio','Julio','Agosto','Septiembre','Octubre','Noviembre','Diciembre']\n",
    "# Etiquetas de los dias de la semana\n",
    "WEEKDAY_TAGS = ['Lunes','Martes','Miércoles','Jueves','Viernes','Sábado','Domingo']\n",
    "# Etiquetas de los momentos del dia\n",
    "# DAYTIME_TAGS = ['Madrugada','Mañana','Mediodía','Tarde','Anochecer','Noche']\n",
    "DAYTIME_TAGS = ['Madrugada','Mañana','Tarde','Noche']\n",
    "# Etiquetas de tendencia\n",
    "TREND_TAGS = ['Igual', 'Creciente', 'Decreciente']\n",
    "# Etiquetas de diagnostico\n",
    "HEALTHY_RANGE = [70,180] # https://doi.org/10.2337/dc22-S006\n",
    "DIAGNOSTIC_TAGS = ['En rango','Por encima de rango','Por debajo de rango']\n",
    "\n",
    "# Columnas del DataFrame\n",
    "INITIAL_COLUMNS = ['Patient_ID',\n",
    "                'Measurement_date',\n",
    "                'Measurement_time',\n",
    "                'Measurement'\n",
    "]\n",
    "\n",
    "FINAL_COLUMNS =['Patient_ID',\n",
    "                'Measurement_date',\n",
    "                'Measurement_time',                \n",
    "                'Difference',\n",
    "                'Trend',\n",
    "                'Diagnostic',\n",
    "                'Measurement_datetime',\n",
    "                'Time_diff',\n",
    "                'Unix_datetime',\n",
    "                'Year',\n",
    "                'Month',\n",
    "                'Day',\n",
    "                'Weekday',\n",
    "                'Hour',\n",
    "                'Minute',\n",
    "                'Daytime',\n",
    "                'Measurement'\n",
    "]\n",
    "TARGET_FEATURE = 'Normalized_Measurement'\n",
    "\n",
    "# Carpeta de destino de los recursos generados\n",
    "OUTPUT_FOLDER = os.path.join(os.getcwd(),'Output')\n",
    "DATA_FOLDER = os.path.join(os.getcwd(),'Data')\n",
    "MODELS_FOLDER = os.path.join(DATA_FOLDER,'Models')\n",
    "# Capeta de destino del dataframe de series temporales\n",
    "TIME_SERIES_PATH = os.path.join(DATA_FOLDER,\"Glucose_Time_Series.csv\")\n",
    "PREDICTIONS_PATH = os.path.join(DATA_FOLDER,\"Predictions_test.csv\")\n",
    "SUMMARY_PATH = os.path.join(DATA_FOLDER,\"Summary.csv\")\n",
    "\n",
    "# Crear en caso que no exista previamente\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "if not os.path.exists(MODELS_FOLDER):\n",
    "    os.makedirs(MODELS_FOLDER)\n",
    "\n",
    "# Color azul UNIR\n",
    "UNIR_COLOR = '#0098CD'\n",
    "\n",
    "# Longitud de las secuencias temporales\n",
    "SEQUENCE_LENGTH = 12\n",
    "\n",
    "# Rango de tiempo permitido entre los registros de una misma secuencia\n",
    "TIME_DIFF_RANGE = [14,15,16]\n",
    "\n",
    "# Definir el modo de representación de los valores en Pandas\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Tamaño del conjunto de training sobre el conjunto de datos completo\n",
    "TRAIN_RATIO = 0.80\n",
    "# Tamaño del conjunto de test sobre el conjunto de datos completo\n",
    "TEST_RATIO = 0.20\n",
    "# Tamaño del conjunto de validacion sobre el conjunto test\n",
    "VALIDATION_RATIO = 0.5\n",
    "# Instanciar SCALER para normalizar los datos entre 0 y 1\n",
    "SCALER = MinMaxScaler()\n",
    "\n",
    "# Etiquetas de las zonas de la rejilla de errores de Clarke\n",
    "ZONE_LABELS = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "ZONE_DESCRIPTIONS = {\n",
    "    'A': \"dentro del 20% del sensor de referencia\",\n",
    "    'B': \"fuera del 20% pero no conduciría a un tratamiento inapropiado\",\n",
    "    'C': \"que indican un tratamiento innecesario\",\n",
    "    'D': \"que indican un potencial fallo peligroso en la detección de hipoglucemia o hiperglucemia\",\n",
    "    'E': \"que podrían confundir el tratamiento de hipoglucemia con hiperglucemia y viceversa\"\n",
    "}\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d95d612",
   "metadata": {},
   "source": [
    "## Definición de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9022af",
   "metadata": {},
   "source": [
    "### Número de mediciones por paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurementsPerPatient(dataframe):\n",
    "    unique_values = dataframe['Patient_ID'].unique()\n",
    "    print('Número de pacientes: {}'.format(len(unique_values)))\n",
    "    examples_per_patient = dataframe['Patient_ID'].value_counts()\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(examples_per_patient.index, examples_per_patient.values, color=UNIR_COLOR)\n",
    "    plt.title('Número de mediciones por paciente')\n",
    "    plt.xlabel('Paciente')\n",
    "    plt.ylabel('Número de mediciones')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlim(-0.6, len(examples_per_patient) - 0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d0084",
   "metadata": {},
   "source": [
    "### Número de mediciones por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53133b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurementPerYear(dataframe):\n",
    "    temp_df = dataframe.copy()\n",
    "    temp_df['Measurement_date'] = pd.to_datetime(temp_df['Measurement_date'])\n",
    "    print('Fecha del primer registro: {}\\nFecha del último registro {}'.format(str(temp_df['Measurement_date'].min()).split(' ')[0],str(temp_df['Measurement_date'].max()).split(' ')[0]))\n",
    "    year_counts = temp_df['Measurement_date'].dt.year.value_counts()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(year_counts.index, year_counts.values, color=UNIR_COLOR)\n",
    "    plt.xlabel('Año')\n",
    "    plt.ylabel('Número de mediciones')\n",
    "    plt.grid(True)\n",
    "    plt.title('Número de mediciones por año')\n",
    "    for i, value in enumerate(year_counts.values):\n",
    "        plt.text(year_counts.index[i], value, str(value), ha='center', va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172492c9",
   "metadata": {},
   "source": [
    "### Número de mediciones por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthlyMeasurement(dataframe):\n",
    "    temp_df = dataframe.copy()\n",
    "    temp_df['Measurement_date'] = pd.to_datetime(temp_df['Measurement_date'])\n",
    "    month_counts = temp_df['Measurement_date'].dt.month.value_counts()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(month_counts.index, month_counts.values, color=UNIR_COLOR)\n",
    "    plt.xlabel('Mes')\n",
    "    plt.ylabel('Número de mediciones')\n",
    "    plt.grid(True)\n",
    "    plt.title('Número de mediciones por mes')\n",
    "    plt.xticks(range(1, 13), [MONTH_TAGS[x-1] for x in range(1, 13)],rotation='vertical')\n",
    "    for i, value in enumerate(month_counts.values):\n",
    "        plt.text(month_counts.index[i], value, str(value), ha='center', va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b2abfb",
   "metadata": {},
   "source": [
    "### Número de mediciones por mes y año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b102aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthlyMeasurementPerYear(dataframe):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    temp_df = dataframe.copy()\n",
    "    temp_df['Measurement_date'] = pd.to_datetime(temp_df['Measurement_date'])    \n",
    "    years = temp_df['Measurement_date'].dt.year.unique()\n",
    "    years_sorted = sorted(years)  \n",
    "    for year in years_sorted:\n",
    "        year_data = temp_df[temp_df['Measurement_date'].dt.year == year]\n",
    "        month_counts = year_data['Measurement_date'].dt.month.value_counts().sort_index()\n",
    "        plt.plot(month_counts.index, month_counts.values, marker='o', label=str(year))\n",
    "    plt.xlabel('Mes')\n",
    "    plt.ylabel('Número de mediciones')\n",
    "    plt.title('Número de mediciones por mes y año')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(range(1, 13), [MONTH_TAGS[x-1] for x in range(1, 13)], rotation='vertical')\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9bd02",
   "metadata": {},
   "source": [
    "### Número de mediciones por dia de la semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekdayMeasurement(dataframe):\n",
    "    weekday_means = []    \n",
    "    for weekday in range(len(WEEKDAY_TAGS)):\n",
    "        sample = dataframe[dataframe['Weekday']==weekday]\n",
    "        weekday_means.append(round(sample['Measurement'].describe()['mean'],2))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(WEEKDAY_TAGS,weekday_means, color=UNIR_COLOR)\n",
    "    plt.xlabel('Día de la semana')\n",
    "    plt.ylabel('Media del nivel de glucosa')\n",
    "    plt.title('Media del nivel de glucosa por día de la semana')    \n",
    "    for i,mean in enumerate(weekday_means):\n",
    "        plt.text(i,mean,str(mean),ha='center',va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549e384",
   "metadata": {},
   "source": [
    "### Número de mediciones por momento del dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c471ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daytimeMeasurement(dataframe):\n",
    "    daytime_means = []\n",
    "    for daytime in range(len(DAYTIME_TAGS)):\n",
    "        sample = dataframe[dataframe['Daytime']==daytime]\n",
    "        daytime_means.append(round(sample['Measurement'].describe()['mean'],2))    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(DAYTIME_TAGS, daytime_means, marker='o', color=UNIR_COLOR)\n",
    "    plt.xlabel('Momento del día')\n",
    "    plt.ylabel('Media del nivel de glucosa')\n",
    "    plt.title('Media del nivel de glucosa por momento del día')\n",
    "    for i, mean in enumerate(daytime_means):\n",
    "        plt.text(i, mean, str(mean), ha='center', va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7c8f2",
   "metadata": {},
   "source": [
    "### Crear matriz de correlación de Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ea9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCorrelation(dataframe, columns_order,name):\n",
    "    dataframe.reindex(columns=columns_order)\n",
    "    dataframe = dataframe.drop('Patient_ID', axis=1)\n",
    "    dataframe['Measurement_date'] = pd.to_datetime(dataframe['Measurement_date'])\n",
    "    dataframe['Measurement_time'] = pd.to_datetime(dataframe['Measurement_time'])\n",
    "    correlation_matrix = dataframe.corr()\n",
    "    measurement_correlation = dataframe.corr()['Measurement']\n",
    "    correlation_matrix_filled = correlation_matrix.copy()\n",
    "    correlation_matrix_filled['Measurement'] = measurement_correlation\n",
    "    correlation_matrix_filled.loc['Measurement'] = measurement_correlation\n",
    "      \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix_filled, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title(\"Matriz de Correlación de Pearson del conjunto de datos {}\".format(name))\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(OUTPUT_FOLDER, 'pearson_{}.png'.format(name))) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2134a6a",
   "metadata": {},
   "source": [
    "### Crear gfráficas del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e792a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTraining(model_name,history): \n",
    "    train_loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(train_loss, label='Entrenamiento ', marker='o', color=UNIR_COLOR)\n",
    "    plt.plot(val_loss, label='Validación', marker='o', color='#FF6732')\n",
    "    plt.title('Pérdida (MSE) en Entrenamiento y Validación Loss\\n{}'.format(model_name))\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    min_train_loss = min(train_loss)\n",
    "    max_train_loss = max(train_loss)\n",
    "    min_val_loss = min(val_loss)\n",
    "    max_val_loss = max(val_loss)\n",
    "\n",
    "    pad = 0.00002\n",
    "    plt.text(train_loss.index(min_train_loss), min_train_loss+pad, '{:.4f} min E'.format(min_train_loss), ha='center', va='bottom', color=UNIR_COLOR)\n",
    "    plt.text(train_loss.index(max_train_loss), max_train_loss+pad, '{:.4f} max E'.format(max_train_loss), ha='center', va='bottom', color=UNIR_COLOR)\n",
    "    \n",
    "    plt.text(val_loss.index(min_val_loss), min_val_loss-pad, '{:.4f} min V'.format(min_val_loss), ha='center', va='top', color='#FF6732')\n",
    "    plt.text(val_loss.index(max_val_loss), max_val_loss-pad, '{:.4f} max V'.format(max_val_loss), ha='center', va='top', color='#FF6732')\n",
    "\n",
    "    # Adjust the y-axis limits\n",
    "    plt.ylim(min(min_train_loss, min_val_loss)-0.0001, max(max_train_loss, max_val_loss)+0.0001)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_FOLDER, 'training_{}.png'.format(model_name))) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05486e13",
   "metadata": {},
   "source": [
    "### Rejilla de error de Clarke (adaptada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DISCLAIMER:\n",
    "La función 'clarke_error_grid' ha sido recopilada del GitHub de Trevor Tsue y adaptada para la representación del trabajo actual\n",
    "# https://github.com/suetAndTie/ClarkeErrorGrid/blob/master/ClarkeErrorGrid.py\n",
    "\n",
    "'''\n",
    "\n",
    "def clarke_error_grid(ref_values, pred_values, model_name):\n",
    "    # Comprobando si las longitudes de los arrays de referencia y predicción son iguales\n",
    "    assert (len(ref_values) == len(pred_values)), \"Número desigual de valores (referencia: {}) (predicción: {}).\".format(len(ref_values), len(pred_values))\n",
    "    # Limpiar el gráfico\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # Configurar el gráfico\n",
    "    plt.scatter(ref_values, pred_values, marker='o', color='#0098CD', s=8, alpha=0.25)\n",
    "    plt.title(model_name + \" Gráfico de Error de Clarke\")\n",
    "    plt.xlabel(\"Concentración de Referencia (mg/dl)\")\n",
    "    plt.ylabel(\"Concentración de Predicción (mg/dl)\")\n",
    "    plt.xticks([0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550])\n",
    "    plt.yticks([0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550])\n",
    "    plt.gca().set_facecolor('white')\n",
    "\n",
    "    # Establecer las longitudes de los ejes\n",
    "    plt.gca().set_xlim([0, 550])\n",
    "    plt.gca().set_ylim([0, 550])\n",
    "    plt.gca().set_aspect((400)/(400))\n",
    "\n",
    "    plt.plot([0, 550], [0, 550], ':', c='black')  # Línea de regresión teórica de 45 grados\n",
    "    plt.plot([0, 175/3], [70, 70], '-', c='black')\n",
    "    plt.plot([175/3, 550/1.2], [70, 550], '-', c='black')  # Reemplazar 320 con 400/1.2 porque 100*(400 - 400/1.2)/(400/1.2) =  20% de error\n",
    "    plt.plot([70, 70], [84, 550], '-', c='black')\n",
    "    plt.plot([0, 70], [180, 180], '-', c='black')\n",
    "    plt.plot([70, 440], [180, 550], '-', c='black')\n",
    "    plt.plot([70, 70], [0, 56], '-', c='black')  # Reemplazar 175.3 con 56 porque 100*abs(56-70)/70) = 20% de error\n",
    "    plt.plot([70, 550], [56, 440], '-', c='black')\n",
    "    plt.plot([180, 180], [0, 70], '-', c='black')\n",
    "    plt.plot([180, 550], [70, 70], '-', c='black')\n",
    "    plt.plot([240, 240], [70, 180], '-', c='black')\n",
    "    plt.plot([240, 550], [180, 180], '-', c='black')\n",
    "    plt.plot([130, 180], [0, 70], '-', c='black')\n",
    "\n",
    "    # Agregar títulos de zonas\n",
    "    plt.text(30, 15, \"A\", fontsize=15)\n",
    "    plt.text(370, 260, \"B\", fontsize=15)\n",
    "    plt.text(280, 370, \"B\", fontsize=15)\n",
    "    plt.text(160, 370, \"C\", fontsize=15)\n",
    "    plt.text(160, 15, \"C\", fontsize=15)\n",
    "    plt.text(30, 140, \"D\", fontsize=15)\n",
    "    plt.text(370, 120, \"D\", fontsize=15)\n",
    "    plt.text(30, 370, \"E\", fontsize=15)\n",
    "    plt.text(370, 15, \"E\", fontsize=15)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Estadísticas de los datos\n",
    "    zone = [0] * 5\n",
    "    zones = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    for i in range(len(ref_values)):\n",
    "        if (ref_values[i] <= 70 and pred_values[i] <= 70) or (pred_values[i] <= 1.2 * ref_values[i] and pred_values[i] >= 0.8 * ref_values[i]):\n",
    "            zone[0] += 1  # Zona A\n",
    "            zones['A'] += 1\n",
    "        elif (ref_values[i] >= 180 and pred_values[i] <= 70) or (ref_values[i] <= 70 and pred_values[i] >= 180):\n",
    "            zone[4] += 1  # Zona E\n",
    "            zones['E'] += 1\n",
    "        elif ((ref_values[i] >= 70 and ref_values[i] <= 440) and pred_values[i] >= ref_values[i] + 110) or ((ref_values[i] >= 130 and ref_values[i] <= 180) and (pred_values[i] <= (7 / 5) * ref_values[i] - 182)):\n",
    "            zone[2] += 1  # Zona C\n",
    "            zones['C'] += 1\n",
    "        elif (ref_values[i] >= 240 and (pred_values[i] >= 70 and pred_values[i] <= 180)) or (ref_values[i] <= 175 / 3 and pred_values[i] <= 180 and pred_values[i] >= 70) or ((ref_values[i] >= 175 / 3 and ref_values[i] <= 70) and pred_values[i] >= (6 / 5) * ref_values[i]):\n",
    "            zone[3] += 1  # Zona D\n",
    "            zones['D'] += 1\n",
    "        else:\n",
    "            zone[1] += 1  # Zona B\n",
    "            zones['B'] += 1\n",
    "\n",
    "\n",
    "    plt.savefig(os.path.join(OUTPUT_FOLDER, 'clarke_{}.png'.format(model_name))) \n",
    "    return plt, zones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d28c9e",
   "metadata": {},
   "source": [
    "### Añadir variables extraidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b88720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addAdditionalVariables(dataframe, verbose=False):\n",
    "\t# Concatenar fecha y hora\n",
    "    dataframe['Measurement_datetime'] = pd.to_datetime(dataframe['Measurement_date'] + ' ' + dataframe['Measurement_time'])\n",
    "\n",
    "\n",
    "    print('Añadiendo variables inferidas de la medición:\\n\\t\"Difference\", \"Trend\", \"Diagnostic\"')\n",
    "\t# Convierte la columna \"Measurement\" a tipo numérico\n",
    "    dataframe[\"Measurement\"] = pd.to_numeric(dataframe[\"Measurement\"], errors=\"coerce\")\n",
    "\n",
    "\t# Ordena el DataFrame por fecha y hora dentro de cada paciente\n",
    "    dataframe = dataframe.groupby(\"Patient_ID\", group_keys=False).apply(lambda x: x.sort_values([\"Measurement_datetime\"]))\n",
    "\n",
    "\t# Calcula la diferencia de medición entre registros consecutivos para cada paciente\n",
    "    dataframe[\"Difference\"] = dataframe.groupby(\"Patient_ID\", group_keys=False)[\"Measurement\"].diff().fillna(0)\n",
    "    dataframe['Difference'] = pd.to_numeric(dataframe['Difference']).astype(int)\n",
    "\n",
    "\t# Calcula la tendencia de la medición (1 para diferencia positiva, -1 para diferencia negativa, 0 para ninguna diferencia)\n",
    "    dataframe['Trend'] = np.sign(dataframe['Difference']).astype(int)\n",
    "\n",
    "\t# Definir diagnostico según nivel de glucosa (1 para Hiperglucemia (más de 130), -1 para Hipoglucemia (menos de 70), 0 para nivel de glucemia controlado)\n",
    "    dataframe['Diagnostic'] = np.where((dataframe['Measurement'] > HEALTHY_RANGE[1]), 1,np.where((dataframe['Measurement'] < HEALTHY_RANGE[0]), -1, 0)).astype(int)\n",
    "    print('Añadiendo variables inferidas de la fecha:\\n\\t\"Year\", \"Month\", \"Day\", \"Weekday\"')\n",
    "\n",
    "\t# Extrae componentes de fecha\n",
    "    dataframe['Year'] = dataframe['Measurement_datetime'].dt.year\n",
    "    dataframe['Month'] = dataframe['Measurement_datetime'].dt.month\n",
    "    dataframe['Day'] = dataframe['Measurement_datetime'].dt.day\n",
    "\n",
    "\t# Calcula el día de la semana según la fecha\n",
    "    dataframe['Weekday'] = dataframe['Measurement_datetime'].dt.weekday\n",
    "    print('Añadiendo variables inferidas de la hora:\\n\\t\"Hour\", \"Minute\", \"Daytime\"')\n",
    "\n",
    "\t# Extrae componentes de tiempo\n",
    "    dataframe['Hour'] = dataframe['Measurement_datetime'].dt.hour\n",
    "    dataframe['Minute'] = dataframe['Measurement_datetime'].dt.minute\n",
    "\n",
    "\t# Etiqueta el momento del día\n",
    "\n",
    "\t# DAYTIME_TAGS = ['0-4', '4-10', '10-13', '13-18', '18-22', '22-24']\n",
    "    # dataframe['Daytime'] = pd.cut(dataframe['Hour'], bins=[-1, 4, 10, 13, 18, 22, 24], labels=np.arange(len(DAYTIME_TAGS))).astype(int)\n",
    "    dataframe['Daytime'] = pd.cut(dataframe['Hour'], bins=[-1, 6, 12, 19, 24], labels=np.arange(len(DAYTIME_TAGS))).astype(int)\n",
    "    print('Añadiendo fecha y hora en formato Unix:\\n\\t\"Unix_datetime\"')\n",
    "\n",
    "\t# Convierte la fecha y hora a formato Unix como identificador de tiempo para secuencias temporales\n",
    "    dataframe['Unix_datetime'] = dataframe['Measurement_datetime'].apply(lambda x: int(x.timestamp()))\n",
    "    print('Añadiendo diferencia de tiempo entre registros anteriores por paciente:\\n\\t\"Time_diff\"')\n",
    "    dataframe['Time_diff'] = dataframe.groupby('Patient_ID', group_keys=False)['Measurement_datetime'].diff().dt.total_seconds()//60\n",
    "    dataframe['Time_diff'].fillna(15,inplace=True)\n",
    "\t# Los NA corresponden al primer registro de cada paciente, por eso lo establecemos en 15 y asi no lo obviamos en posteriores procesamientos\n",
    "    dataframe['Time_diff'] = dataframe['Time_diff'].astype(int)\n",
    "\n",
    " \n",
    "    return dataframe.reindex(columns=FINAL_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e4311",
   "metadata": {},
   "source": [
    "### Crear series temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c962c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tratar valores del primer registro despues de una secuencia eliminada (Diferencia de tiempo mayor a 15 min)\n",
    "\n",
    "def createTemporalSeries(dataframe, verbose=False):\n",
    "    print(\"Creando series temporales\")\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "    # Crear diccionario vacio    \n",
    "    valid_indexes = []\n",
    "    # Agrupar los registros del conjunto de datos por paciente\n",
    "    for patient_id, patient_data in dataframe.groupby(\"Patient_ID\"):\n",
    "        # print(patient_id, len(patient_data))\n",
    "        current_sequence=[]\n",
    "        for index, row in patient_data.iterrows():\n",
    "            if row['Time_diff'] in TIME_DIFF_RANGE:\n",
    "                # Comprobar longitud de la secuencia actual\n",
    "                # Si la secuencia actual tiene 12 registros\n",
    "                if len(current_sequence)==SEQUENCE_LENGTH:\n",
    "                    # Guardar secuencia y añadir registro a nueva secuencia\n",
    "                    valid_indexes.extend(current_sequence)\n",
    "                    current_sequence=[]\n",
    "                # Agregar indice a la secuencia actual\n",
    "                current_sequence.append(index)\n",
    "            else:\n",
    "                current_sequence=[]\n",
    "\n",
    "    return dataframe[dataframe.index.isin(valid_indexes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d344394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4953f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17fafd8a",
   "metadata": {},
   "source": [
    "### Entreno de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitAndEvaluate(model,model_name,training_set,validation_set,test_set, epochs=10, batch_size=32):\n",
    "    X_train,y_train = training_set\n",
    "    X_validation,y_validation = validation_set\n",
    "    X_test,y_test = test_set\n",
    "\n",
    "    model_path = os.path.join(MODELS_FOLDER,'{}.h5'.format(model_name))\n",
    "    history_path = model_path.replace('.h5','.pkl')\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"\\nEmpezando el entrenamiento del modelo {}\".format(model_name))\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), epochs=epochs, batch_size=batch_size)        \n",
    "        model.save(model_path)\n",
    "\n",
    "        with open(history_path, 'wb') as history_file:\n",
    "            pickle.dump(history.history, history_file)\n",
    "        print('Modelo \"{}\" guardado en:\\n\\t{}\\nHistorial de entrenamiento guardado en:\\n\\t{}'.format(model_name,model_path,history_path))\n",
    "        history_data = history.history        \n",
    "    else:\n",
    "        print('\\nEl modelo \"{}\" ya existe. \\nCargando modelo e historial de entrenamiento.'.format(model_name))\n",
    "        model = load_model(model_path)\n",
    "        with open(history_path,'rb') as history_file:\n",
    "            history_data = pickle.load(history_file)      \n",
    "        \n",
    "    plotTraining(model_name, history_data)\n",
    "\n",
    "    print('\\n==== Resultados de evaluación del modelo ====\\n')\n",
    "    loss = model.evaluate(X_test,y_test)\n",
    "    print('Pérdida (MSE) : {}'.format(loss))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "    y_pred = SCALER.inverse_transform(y_pred)\n",
    "    y_test = SCALER.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print('\\n==== Resultados de test ====\\n')\n",
    "    print('Error Cuadrático Medio (MSE): {}'.format(mse))\n",
    "    print('Error Absoluto Medio (MAE): {}'.format(mae))\n",
    "    print('Puntuación R^2: {}'.format(r2))  \n",
    "\n",
    "    plot,zones = clarke_error_grid(y_test, y_pred, model_name)\n",
    "    plot.show()\n",
    "    for zone_label in zones:\n",
    "        if zone_label in ZONE_DESCRIPTIONS:        \n",
    "            print(\"Región {}: {} puntos {}\".format(zone_label,zones[zone_label],ZONE_DESCRIPTIONS[zone_label]))\n",
    "        else:        \n",
    "            print(\"Región {}: {} puntos\".format(zone_label,zones[zone_label]))\n",
    "    print(\"Total casos: {}\".format(len(y_pred)))\n",
    "\n",
    "    return [loss,mse,mae,r2,y_pred] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c7d66",
   "metadata": {},
   "source": [
    "### Crear sets de entrenamiento, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainValTest(dataframe, variables=['Measurement'],case=''):\n",
    "    print(\"Creando conjuntos de Entrenamiento, Validación y Test\")\n",
    "    if 'Measurement' not in variables:\n",
    "        variables.append('Measurement')\n",
    "\n",
    "    for variable in variables:\n",
    "        dataframe.loc[:, 'Normalized_{}'.format(variable)] = SCALER.fit_transform(dataframe[variable].values.reshape(-1, 1))\n",
    "\n",
    "    norm_names = list(map(lambda x: 'Normalized_{}'.format(x), variables))\n",
    "    new_dataframe = dataframe[norm_names].copy().reset_index(drop=True)\n",
    "    # print(new_dataframe.head())\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    tags = 0\n",
    "    k=0\n",
    "    current_sequence = []\n",
    "    for i, row in new_dataframe.iterrows():\n",
    "        # print('{}[{}] : {}'.format(i,k,row['Normalized_Measurement']))        \n",
    "        current_sequence.append(row.values.flatten().tolist())\n",
    "        if (k%(SEQUENCE_LENGTH-1)==0 and k>0) or k==len(new_dataframe):\n",
    "            # print('---SAVING LABEL {} AT INDEX {}'.format(row['Normalized_Measurement'],i))            \n",
    "            k=0\n",
    "            sequences.append(current_sequence)\n",
    "            labels.append(row['Normalized_Measurement'])\n",
    "            current_sequence = []\n",
    "        else:\n",
    "            k+=1\n",
    "\n",
    "    print(\"Shape of sequences after split: \", np.array(sequences).shape)\n",
    "    print(\"Shape of labels after split: \", np.array(labels).shape)\n",
    "\n",
    "    print(sequences[0])\n",
    "    print(labels[0])\n",
    "\n",
    "        # X_train, X_temp, y_train, y_temp\n",
    "    train_sequences, temp_sequences, train_labels, temp_labels = train_test_split(sequences, labels, test_size=TEST_RATIO, random_state=12081995)\n",
    "    # X_test, X_val, y_test, y_val\n",
    "    test_sequences, val_sequences, test_labels, val_labels = train_test_split(temp_sequences, temp_labels, test_size=VALIDATION_RATIO, random_state=12081995)\n",
    "\n",
    "    X_train,y_train = [train_sequences,train_labels]\n",
    "    X_validation,y_validation = [val_sequences,val_labels]\n",
    "    X_test,y_test = [test_sequences,test_labels]\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_validation = np.array(X_validation)\n",
    "    y_validation = np.array(y_validation)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], SEQUENCE_LENGTH, len(variables))\n",
    "    X_validation = X_validation.reshape(X_validation.shape[0], SEQUENCE_LENGTH, len(variables))\n",
    "    X_test=X_test.reshape(X_test.shape[0], SEQUENCE_LENGTH, len(variables))\n",
    "\n",
    "    total_size = len(sequences)\n",
    "    print(\"Secuencias totales: {} ({} registros)\".format(total_size,total_size*SEQUENCE_LENGTH))\n",
    "    train_percent = int(((total_size*TRAIN_RATIO)/total_size)*100)\n",
    "    test_percent  = int((((total_size*TEST_RATIO)/total_size)*100)*0.5)\n",
    "    val_percent   = test_percent\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "\n",
    "    print(\"Secuencias de entrenamiento: {} ({}%) - Etiquetas de entrenamiento: {}\\nPrimera secuencia: \\n{}\\nEtiqueta: {}\\n\".format(len(X_train),train_percent,len(y_train),X_train[0].flatten(),y_train[0]))\n",
    "    print(\"Secuencias de validación: {} ({}%) - Etiquetas de validación: {}\\nPrimera secuencia: \\n{}\\nEtiqueta: {}\\n\".format(len(X_validation),val_percent,len(y_validation),X_validation[0].flatten(),y_validation[0])) \n",
    "    print(\"Secuencias de  prueba: {} ({}%) - Etiquetas de prueba: {}\\nPrimera secuencia: \\n{}\\nEtiqueta: {}\\n\".format(len(X_test),test_percent,len(y_test),X_test[0].flatten(),y_test[0]))\n",
    "    \n",
    "    # with open(os.path.join(DATA_FOLDER,'{}.pkl'.format(case)),'wb') as bf:\n",
    "    #     pickle.dump([(X_train, y_train),(X_validation, y_validation),(X_test, y_test),SCALER],bf)\n",
    "\n",
    "    return [X_train,y_train],[X_validation,y_validation],[X_test,y_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c796ebd",
   "metadata": {},
   "source": [
    "### Run cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCases(dataframe,cases_summary):\n",
    "    optimizer = Adam(learning_rate=0.001,decay=1e-6)\n",
    "    for case in cases_summary:\n",
    "        variables = cases_summary[case][0]\n",
    "        print('\\n\\n\\n')\n",
    "        print('\\n{}\\n===== {} =====\\n{}\\nVariables usadas: {}'.format('='*95,case,'='*95,variables))\n",
    "        training_set,validation_set,test_set = splitTrainValTest(dataframe,variables, case)\n",
    "        # if not os.path.exists(os.path.join(DATA_FOLDER,'{}.pkl'.format(case))):\n",
    "        #     training_set,validation_set,test_set = splitTrainValTest(dataframe,variables, case)\n",
    "        # else:\n",
    "            \n",
    "        #     print('Cargando conjuntos de entrenamiento, validación y prueba')\n",
    "        #     with open(os.path.join(DATA_FOLDER,'{}.pkl'.format(case)),'rb')as bf:\n",
    "        #         loaded_data = pickle.load(bf)\n",
    "        #     training_set, validation_set, test_set,SCALER = loaded_data\n",
    "        model_name = '{}{}_M1'.format(case[0],case[-1])\n",
    "        print('{}\\n==== Primer modelo ({}) ====\\n{}'.format('='*16, model_name ,'='*16))\n",
    "\n",
    "        model1 = Sequential()\n",
    "        model1.add(CuDNNLSTM(units=128,input_shape=(SEQUENCE_LENGTH, len(variables))))\n",
    "        model1.add(Dense(units=32, activation=\"linear\"))\n",
    "        model1.add(Dense(1,activation='linear'))\n",
    "        model1.summary()\n",
    "        plot_model(model1, to_file=os.path.join(OUTPUT_FOLDER,'{}_plot.png'.format(model_name)), show_shapes=True, show_layer_names=True)\n",
    "        model1.compile(optimizer=optimizer,loss='mean_squared_error')\n",
    "        cases_summary[case].append(fitAndEvaluate(model=model1,model_name=model_name.format(case),training_set=training_set,validation_set=validation_set,test_set=test_set, epochs=EPOCHS, batch_size=BATCH_SIZE))\n",
    "\n",
    "        model_name = '{}{}_M2'.format(case[0],case[-1])\n",
    "        print('{}\\n==== Segundo modelo ({}) ====\\n{}'.format('='*16, model_name ,'='*16))\n",
    "        model2 = Sequential()\n",
    "        model2.add(CuDNNLSTM(units=256,return_sequences=True,input_shape=(SEQUENCE_LENGTH, len(variables))))\n",
    "        model2.add(Dropout(0.25))\n",
    "        model2.add(CuDNNLSTM(128))\n",
    "        model2.add(Dense(units=64, activation=\"linear\"))\n",
    "        model2.add(BatchNormalization())\n",
    "        model2.add(Dropout(0.25))        \n",
    "        model2.add(Dense(units=32, activation=\"linear\"))\n",
    "        model2.add(BatchNormalization())\n",
    "        model2.add(Dropout(0.25))\n",
    "        model2.add(Dense(1,activation='linear'))\n",
    "        model2.summary()\n",
    "        plot_model(model2, to_file=os.path.join(OUTPUT_FOLDER,'{}_plot.png'.format(model_name)), show_shapes=True, show_layer_names=True)\n",
    "        model2.compile(optimizer=optimizer,loss='mean_squared_error')\n",
    "        cases_summary[case].append(fitAndEvaluate(model=model2,model_name=model_name.format(case),training_set=training_set,validation_set=validation_set,test_set=test_set, epochs=EPOCHS, batch_size=BATCH_SIZE))\n",
    "        \n",
    "        model_name = '{}{}_M3'.format(case[0],case[-1])        \n",
    "        model3 = Sequential()\n",
    "        model3.add(CuDNNLSTM(units=128, input_shape=(SEQUENCE_LENGTH, len(variables))))\n",
    "        model3.add(Dropout(0.2))\n",
    "        model3.add(Dense(units=256, activation='linear'))\n",
    "        model3.add(BatchNormalization())\n",
    "        model3.add(Dropout(0.2))\n",
    "        model3.add(Dense(units=128, activation='linear'))\n",
    "        model3.add(BatchNormalization())\n",
    "        model3.add(Dropout(0.2))\n",
    "        model3.add(Dense(units=64, activation='linear'))\n",
    "        model3.add(BatchNormalization())\n",
    "        model3.add(Dropout(0.2))\n",
    "        model3.add(Dense(units=32, activation='linear'))\n",
    "        model3.add(BatchNormalization())\n",
    "        model3.add(Dropout(0.2))\n",
    "        model3.add(Dense(units=1, activation='linear'))\n",
    "        model3.summary()\n",
    "        plot_model(model3, to_file=os.path.join(OUTPUT_FOLDER,'{}_plot.png'.format(model_name)), show_shapes=True, show_layer_names=True)\n",
    "        model3.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        cases_summary[case].append(fitAndEvaluate(model=model3,model_name=model_name.format(case),training_set=training_set,validation_set=validation_set,test_set=test_set, epochs=EPOCHS, batch_size=BATCH_SIZE))\n",
    "\n",
    "    data = []\n",
    "    prediction_results = [test_set[1]]\n",
    "    predictions_columns = ['y_test']\n",
    "    for case in cases_summary:\n",
    "        variables= cases_summary[case][0]\n",
    "        for i,model in enumerate(cases_summary[case][1:]):            \n",
    "            loss,mse,mae,r2,y_pred = model\n",
    "            data.append([case,\"Model {}\".format(i+1),mse,mae,r2])\n",
    "            prediction_results.append(y_pred)\n",
    "            predictions_columns.append('{}_Model{}'.format(case,i+1))\n",
    "\n",
    "    summary_columns = ['Case', 'Model', 'mse','mae','r2']\n",
    "    df_summary = pd.DataFrame(data, columns=summary_columns)\n",
    "    df_summary.to_csv(SUMMARY_PATH,index=False)\n",
    "\n",
    "    reshaped_prediction_results = [np.reshape(arr, (-1,)) for arr in prediction_results]\n",
    "    transposed_prediction_results = np.transpose(reshaped_prediction_results)\n",
    "    df_prediction_results = pd.DataFrame(data=transposed_prediction_results, columns=predictions_columns)\n",
    "    df_prediction_results.to_csv(PREDICTIONS_PATH,index=False)\n",
    "    return df_summary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39353d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9fb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b3b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f2a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMetricsAnalisys():\n",
    "    data = pd.read_csv(SUMMARY_PATH)\n",
    "    # Obtener la lista de casos y modelos únicos\n",
    "    casos = data['Case'].unique()\n",
    "    modelos = data['Model'].unique()\n",
    "\n",
    "    # Iterar sobre los casos y modelos\n",
    "    for caso in casos:\n",
    "        for modelo in modelos:\n",
    "            # Filtrar los datos por caso y modelo\n",
    "            datos_filtrados = data[(data['Case'] == caso) & (data['Model'] == modelo)]\n",
    "\n",
    "            if not datos_filtrados.empty:\n",
    "                # Obtener las métricas correspondientes\n",
    "                mse = datos_filtrados['mse'].values[0]\n",
    "                mae = datos_filtrados['mae'].values[0]\n",
    "                r2 = datos_filtrados['r2'].values[0]\n",
    "\n",
    "                # Configuración de la figura y el subplot\n",
    "                fig, ax = plt.subplots(figsize=(6, 6))\n",
    "                fig.suptitle(f'Resultados de métricas\\nCaso: {caso}\\nModelo: {modelo}')\n",
    "\n",
    "                # Graficar las métricas correspondientes\n",
    "                ax.bar(['MSE', 'MAE', 'R^2'], [mse, mae, r2])\n",
    "                max_value = max(max(mse,mae),r2)\n",
    "                ax.set_ylim(0, max_value*1.1)  # Aumentar el límite del eje y\n",
    "                ax.set_ylabel('Valor')\n",
    "\n",
    "                # Mostrar los valores en las barras\n",
    "                for index, value in enumerate([mse, mae, r2]):\n",
    "                    ax.text(index, 0.5, str(round(value, 5)), ha='center', va='bottom')\n",
    "                plt.grid(True)\n",
    "                # Mostrar la figura\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6657554",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f72f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_summary = {\n",
    "    'Caso1':[['Measurement']],\n",
    "    'Caso2':[['Month','Day']],\n",
    "    'Caso3':[['Weekday']],\n",
    "    'Caso4':[['Hour','Minute']],\n",
    "    'Caso5':[['Daytime']],\n",
    "    'Caso6':[['Trend','Diagnostic']],\n",
    "    'Caso7':[['Difference']],\n",
    "    'Caso8':[['Month','Day','Weekday','Daytime','Trend','Diagnostic','Difference']]\n",
    "}\n",
    "\n",
    "if not os.path.exists(TIME_SERIES_PATH):\n",
    "    df_glucose = pd.read_csv('Glucose_measurements_sample.csv')\n",
    "    measurementsPerPatient(df_glucose)\n",
    "    monthlyMeasurement(df_glucose)\n",
    "    monthlyMeasurementPerYear(df_glucose)    \n",
    "    plotCorrelation(df_glucose,INITIAL_COLUMNS, \"inicial\")\n",
    "    df_glucose = addAdditionalVariables(df_glucose)    \n",
    "    weekdayMeasurement(df_glucose)\n",
    "    daytimeMeasurement(df_glucose)\n",
    "    plotCorrelation(df_glucose,FINAL_COLUMNS, \"final\")\n",
    "    time_series = createTemporalSeries(df_glucose)\n",
    "    time_series.to_csv(TIME_SERIES_PATH)\n",
    "else:\n",
    "    time_series = pd.read_csv(TIME_SERIES_PATH)\n",
    "    \n",
    "\n",
    "cases_summary = runCases(time_series,cases_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80780d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
